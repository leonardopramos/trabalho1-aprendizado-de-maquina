{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de Árvore de Decisão com NumPy\n",
    "\n",
    "## Prompt Utilizado\n",
    "\n",
    "   - Implemente o algoritmo de árvore de decisão from scratch utilizando apenas a biblioteca NumPy para operações vetoriais. A implementação não pode utilizar outras bibliotecas prontas, o teste do algoritmo deve ser feito no dataset Iris.csv na pasta raiz do projeto. Após a implementação e execução do algoritmo, printe na tela o desempenho do modelo utilizando as seguintes métricas: Matriz de confusão, Acurácia, Recall, Precisão, F1-score. Gere o arquivo arvore-de-decisao.py com esta implementação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Crítica do Código Gerado\n",
    "\n",
    "### Pontos Fortes\n",
    "- *Simplicidade e Clareza:* O código é direto e fácil de entender, com funções bem definidas para cada etapa do processo de construção da árvore de decisão.\n",
    "- *Uso Exclusivo de NumPy:* Atende ao requisito de não usar bibliotecas além de NumPy, demonstrando como operações básicas podem ser combinadas para construir algoritmos de aprendizado de máquina.\n",
    "\n",
    "### Potenciais Problemas/Diferenças\n",
    "- *Eficiência Computacional:* A implementação pode ser menos eficiente em termos de tempo de execução e uso de memória em comparação com bibliotecas otimizadas como scikit-learn.\n",
    "- *Generalização:* A implementação atual não suporta poda da árvore, o que pode levar a overfitting em datasets maiores ou mais complexos.\n",
    "\n",
    "### Opções de Projeto e Impacto\n",
    "- *Critério de Parada Simples:* O critério de parada é baseado apenas na profundidade máxima e no número mínimo de amostras, o que é uma escolha comum, mas pode ser melhorado com técnicas de poda.\n",
    "\n",
    "### Sugestões de Melhorias\n",
    "- *Implementar Poda:* Incluir um mecanismo para poda da árvore poderia ajudar a melhorar a generalização do modelo.\n",
    "- *Otimização do Cálculo de Ganho de Informação:* Considerar maneiras de otimizar o cálculo do ganho de informação, possivelmente através de vetorização mais eficiente ou algoritmos alternativos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos Resultados Obtidos\n",
    "\n",
    "### Resultados\n",
    "- *Matriz de Confusão:*\n",
    "  \n",
    "  [[ 7  0  0]\n",
    "   [ 0 10  1]\n",
    "   [ 0  0 12]]\n",
    "  \n",
    "- *Acurácia:* 0.9667\n",
    "- *Recall:* [1.0, 0.9091, 1.0]\n",
    "- *Precisão:* [1.0, 1.0, 0.9231]\n",
    "- *F1-Score:* [1.0, 0.9524, 0.96]\n",
    "\n",
    "### Interpretação\n",
    "- *Desempenho Geral:* O modelo apresentou um desempenho excelente no dataset de teste, com alta acurácia e valores de F1-Score indicando bom equilíbrio entre precisão e recall.\n",
    "- *Aplicação em Casos Reais:* Embora os resultados sejam promissores no dataset Iris, que é bem comportado e balanceado, a implementação pode não se generalizar tão bem em casos reais com dados mais complexos sem ajustes adicionais, como poda e otimização.\n",
    "- *Validação Cruzada:* Para uma avaliação mais robusta, seria interessante aplicar validação cruzada para verificar a consistência do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Código da Árvore de Decisão com NumPy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Função para carregar o dataset Iris\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values  # Features (4 primeiras colunas)\n",
    "    y = data.iloc[:, -1].values   # Classe (última coluna)\n",
    "    return X, y\n",
    "\n",
    "# Função para calcular a entropia\n",
    "def entropy(y):\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "# Função para dividir o dataset com base em um valor de feature\n",
    "def split_dataset(X, y, feature_index, threshold):\n",
    "    left_idx = np.where(X[:, feature_index] <= threshold)\n",
    "    right_idx = np.where(X[:, feature_index] > threshold)\n",
    "    return X[left_idx], X[right_idx], y[left_idx], y[right_idx]\n",
    "\n",
    "# Função para calcular o ganho de informação\n",
    "def information_gain(X, y, feature_index, threshold):\n",
    "    parent_entropy = entropy(y)\n",
    "    X_left, X_right, y_left, y_right = split_dataset(X, y, feature_index, threshold)\n",
    "\n",
    "    if len(y_left) == 0 or len(y_right) == 0:\n",
    "        return 0\n",
    "\n",
    "    n = len(y)\n",
    "    n_left, n_right = len(y_left), len(y_right)\n",
    "    weighted_avg_entropy = (n_left / n) * entropy(y_left) + (n_right / n) * entropy(y_right)\n",
    "    \n",
    "    return parent_entropy - weighted_avg_entropy\n",
    "\n",
    "# Nó da árvore de decisão\n",
    "class DecisionNode:\n",
    "    def _init_(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Implementação da árvore de decisão\n",
    "class DecisionTree:\n",
    "    def _init_(self, min_samples_split=2, max_depth=100):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Condições de parada\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Encontrar o melhor split\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                gain = information_gain(X, y, feature_index, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature_index\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        if best_gain == 0:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Dividir o dataset\n",
    "        X_left, X_right, y_left, y_right = split_dataset(X, y, split_idx, split_threshold)\n",
    "\n",
    "        # Crescer as sub-árvores\n",
    "        left_child = self._grow_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return DecisionNode(split_idx, split_threshold, left_child, right_child)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "# Funções para calcular as métricas\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "    for i, true_label in enumerate(classes):\n",
    "        for j, pred_label in enumerate(classes):\n",
    "            matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
    "    return matrix\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / np.sum(cm, axis=0)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / np.sum(cm, axis=1)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "# Carregar o dataset\n",
    "X, y = load_data('Iris.csv')\n",
    "\n",
    "# Codificar as classes para inteiros\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([class_mapping[label] for label in y])\n",
    "\n",
    "# Embaralhar os dados\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "X, y = X[indices], y[indices]\n",
    "\n",
    "# Dividir em treino e teste (80% treino, 20% teste)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Treinar a árvore de decisão\n",
    "tree = DecisionTree(max_depth=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nAcurácia:\", accuracy(y_test, y_pred))\n",
    "print(\"Recall:\", recall(y_test, y_pred))\n",
    "print(\"Precisão:\", precision(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
